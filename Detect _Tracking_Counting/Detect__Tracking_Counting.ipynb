{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5a5qd/IueAgr5Kq2Wj2cQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aleksandr62aa/AI_ML_DL/blob/main/Detect__Tracking_Counting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect, count persons an image | Detect, track, count persons an video and DeepSORT**"
      ],
      "metadata": {
        "id": "Jrj6kML53kpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup YOLO\n",
        "!pip install ultralytics\n",
        "from ultralytics import YOLO\n",
        "import ultralytics\n",
        "ultralytics.__version__"
      ],
      "metadata": {
        "id": "Yr29pp3T34sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the Github Repo**"
      ],
      "metadata": {
        "id": "yulHslzj4rqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Aleksandr62aa/Object_Tracking_DeepSORT_YOLOv8.git\n",
        "%cd Object_Tracking_DeepSORT_YOLOv8"
      ],
      "metadata": {
        "id": "OTNzJqr76stQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ead8f0-b568-437f-9b51-da0399dcb056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Object_Tracking_DeepSORT_YOLOv8'...\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 173 (delta 37), reused 0 (delta 0), pack-reused 97\u001b[K\n",
            "Receiving objects: 100% (173/173), 58.77 MiB | 35.40 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "/content/Object_Tracking_DeepSORT_YOLOv8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect, count persons an image**\n",
        "\n",
        "**Object detector YOLO + count persons**"
      ],
      "metadata": {
        "id": "XIFkPj0cbm9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import time\n",
        "import torch\n",
        "import cv2\n",
        "import torch.backends.cudnn as cudnn\n",
        "from PIL import Image\n",
        "import colorsys\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Pr-mvfdPJ2sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class names YOLO\n",
        "class_names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "               'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "               'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "               'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "               'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "               'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
        "               'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
        "\n",
        "unique_ids = set()"
      ],
      "metadata": {
        "id": "rEV89tiwEnht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a pretrained model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# detect objects in image using a model YOLO\n",
        "results = model(\"/content/Persons2.jpg\", classes=0, conf=0.8, save=True)\n",
        "\n",
        "# parsing model parameters\n",
        "result = results[0]\n",
        "boxes = result.boxes  # boxes object\n",
        "\n",
        "# Object label (class name)\n",
        "class_names = [result.names[box.cls.item()] for box in boxes] # box with cls format, (1, N)\n",
        "# print(\"Class name -->\", class_names)\n",
        "\n",
        "# Confidence object\n",
        "confidences = [round(box.conf.item(), 2)  for box in boxes]   # box with conf format, (1, N)\n",
        "# print(\"Confidence -->\",confidences)\n",
        "\n",
        "# Coordinates of the object's bounding box\n",
        "xyxys = boxes.xyxy # box with xyxy format, (N, 4)\n",
        "xywhs = boxes.xywh # box with xywh format, (N, 4)\n",
        "# print(xywhs)\n",
        "\n",
        "# load a image\n",
        "img = cv2.imread('/content/Persons2.jpg')\n",
        "\n",
        "for id, (xyxy, confidence, class_name) in enumerate(zip(xyxys, confidences, class_names)):\n",
        "    x1, y1, x2, y2 = xyxy\n",
        "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "    # Display the frame of an object in the image\n",
        "    color = (0, 255, 0) # green\n",
        "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), color, 4)\n",
        "\n",
        "    # Display text on an image\n",
        "    text_color = (0, 0, 0)  # Black color for text\n",
        "    cv2.putText(img, f\"{id}  {confidence}\", (int(x1) + 10, int(y1) - 5),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1.0, text_color, 2, cv2.LINE_AA)\n",
        "\n",
        "    # Add id to the set of unique IDs\n",
        "    unique_ids.add(id)\n",
        "\n",
        "\n",
        "# Update the person count based on the number of unique track IDs\n",
        "person_count = len(unique_ids)\n",
        "\n",
        "# Display the number of people in the image\n",
        "text_color = (0, 0, 0)  # Black color for text\n",
        "cv2.putText(img, f\"Person Count: {person_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1, text_color, 2, cv2.LINE_AA)\n",
        "\n",
        "# Save the image - object detector and count persons\n",
        "cv2.imwrite(\"/content/persons2_.jpg\", img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10195ecf-37c2-46a3-86aa-2191de610c4c",
        "id": "Tw6xk4zwAG3H"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 65.3MB/s]\n",
            "\n",
            "image 1/1 /content/Persons2.jpg: 448x640 2 persons, 7.0ms\n",
            "Speed: 2.3ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detect, track, count persons an video and DeepSORT**\n",
        "\n",
        "**Object detector YOLO + DeepSort**"
      ],
      "metadata": {
        "id": "7Nj56Mdd6s3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries DeepSort\n",
        "from deep_sort.utils.parser import get_config\n",
        "from deep_sort.deep_sort import DeepSort\n",
        "from deep_sort.sort.tracker import Tracker\n",
        "\n",
        "deep_sort_weights = 'deep_sort/deep/checkpoint/ckpt.t7'\n",
        "tracker = DeepSort(model_path=deep_sort_weights, max_age=70)"
      ],
      "metadata": {
        "id": "R-9-7bby6s-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the video path\n",
        "video_path = '/content/test3.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "      print(\"Error video file\")\n",
        "\n",
        "# Get the video properties\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "\n",
        "# Output the video path\n",
        "output_path = 'output.avi'\n",
        "out = cv2.VideoWriter(output_path, fourcc, 10, (frame_width, frame_height))\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "CcIAMEJt7QSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "counter, fps, elapsed = 0, 0, 0\n",
        "# Set color values for red, blue, and green\n",
        "red_color = (0, 0, 255)  # (B, G, R)\n",
        "blue_color = (255, 0, 0)  # (B, G, R)\n",
        "green_color = (0, 255, 0)  # (B, G, R)\n",
        "blaсk_color = (0, 0, 0)\n",
        "\n",
        "area1 = np.array([[0, 0],[frame_width, 0],\n",
        "                  [frame_width, frame_height // 2],[0, frame_height // 2]], np.int32)\n",
        "\n",
        "area2 = np.array([[0, frame_height // 2],[frame_width, frame_height // 2],\n",
        "                  [frame_width, frame_height],[0, frame_height]], np.int32)\n",
        "ent = 0\n",
        "ext = 0\n",
        "obj = {}"
      ],
      "metadata": {
        "id": "rhQDyFI07QcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform object detection and tracking using the model**"
      ],
      "metadata": {
        "id": "TGKQXtD-FRMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a model YOLO\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "while cap.isOpened():\n",
        "    # Reading video frames\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if ret:\n",
        "        # detect objects in image using a model YOLO\n",
        "        results = model(frame, classes=0, conf=0.6)\n",
        "        # results = model(frame, classes=(1, 2 ,3, 5, 7), conf=0.8)\n",
        "\n",
        "        # parsing model parameters\n",
        "        result = results[0]\n",
        "        boxes = result.boxes  # Boxes object for bbox outputs\n",
        "\n",
        "        # Confidence object\n",
        "        probs = result.probs\n",
        "        conf = boxes.conf\n",
        "\n",
        "        # Coordinates of the object's bounding box\n",
        "        xyxy = boxes.xyxy\n",
        "        xywh = boxes.xywh\n",
        "\n",
        "        # Object label (class name)\n",
        "        cls = boxes.cls.tolist()  # Convert tensor to list\n",
        "        # for class_index in cls:\n",
        "        #     class_name = class_names[int(class_index)]\n",
        "        #     # print(\"Class:\", class_name)\n",
        "\n",
        "        # #class name\n",
        "        # class_name = [result.names[box.cls.item()] for box in boxes] # box with cls format, (1, N)\n",
        "        # print(\"Class name -->\", class_name)\n",
        "\n",
        "        pred_cls = np.array(cls)\n",
        "        conf = conf.detach().cpu().numpy()\n",
        "        xyxy = xyxy.detach().cpu().numpy()\n",
        "        bboxes_xywh = xywh.cpu().numpy()\n",
        "        bboxes_xywh = np.array(bboxes_xywh, dtype=float)\n",
        "\n",
        "        # object tracking DeepSORT\n",
        "        tracks = tracker.update(bboxes_xywh, conf, frame)\n",
        "\n",
        "        # parsing tracking parameters\n",
        "        for track in tracker.tracker.tracks:\n",
        "\n",
        "            # get track_id object\n",
        "            track_id = track.track_id\n",
        "            hits = track.hits\n",
        "            obj.setdefault(track_id, 0)\n",
        "\n",
        "            # Get bounding box coordinates in (x1, y1, x2, y2) format\n",
        "            x1, y1, x2, y2 = track.to_tlbr()\n",
        "            w = x2 - x1  # Calculate width\n",
        "            h = y2 - y1  # Calculate height\n",
        "            x_obj = int(x1 + w/2)\n",
        "            y_obj = int(y1 + 0)\n",
        "\n",
        "            # Counting object\n",
        "            res_areal1, res_areal2 = -1, -1\n",
        "            if (frame_height // 2) >= y_obj:\n",
        "                res_areal1 = 1\n",
        "            else:\n",
        "                res_areal2 = 1\n",
        "\n",
        "            # res_areal1 = cv2.pointPolygonTest(area1, ((x_obj , y_obj)), False )\n",
        "            # res_areal2 = cv2.pointPolygonTest(area2, ((x_obj , y_obj)), False )\n",
        "\n",
        "            if obj[track_id] == 1:\n",
        "                if res_areal2 == 1:\n",
        "                    obj[track_id] = 2\n",
        "                    ent += 1\n",
        "            elif obj[track_id] == 2:\n",
        "                if res_areal1 == 1:\n",
        "                    obj[track_id] = 1\n",
        "                    ext += 1\n",
        "            elif obj[track_id] == 0:\n",
        "                if res_areal1 == 1:\n",
        "                    obj[track_id] = 1\n",
        "                if res_areal2 == 1:\n",
        "                    obj[track_id] = 2\n",
        "\n",
        "            # Determine color based on track_id\n",
        "            color_id = track_id % 3\n",
        "            if color_id == 0:\n",
        "                color = red_color\n",
        "            elif color_id == 1:\n",
        "                color = blue_color\n",
        "            else:\n",
        "                color = green_color\n",
        "\n",
        "            # display bounding box and text\n",
        "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
        "\n",
        "            cv2.putText(frame, f\"{track_id}\", (int(x1) + 10, int(y1) - 5),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, blaсk_color, 1, cv2.LINE_AA)\n",
        "\n",
        "            cv2.circle(frame,(x_obj, y_obj ), 3, blaсk_color,3)\n",
        "\n",
        "        # Update FPS and place on frame\n",
        "        current_time = time.perf_counter()\n",
        "        elapsed = (current_time - start_time)\n",
        "        counter += 1\n",
        "        if elapsed > 1:\n",
        "            fps = counter / elapsed\n",
        "            counter = 0\n",
        "            start_time = current_time\n",
        "        print('fps', fps)\n",
        "\n",
        "        # display object count and dividing line\n",
        "        cv2.line(frame, (0, frame_height // 2), (frame_width, frame_height // 2), red_color, thickness=2)\n",
        "        # cv2.polylines(frame, [area1], True, red_color, 2)\n",
        "        # cv2.polylines(frame, [area2], True, red_color, 2)\n",
        "\n",
        "        cv2.putText(frame, f'{ent}', (20, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.0, blaсk_color, 3)\n",
        "        cv2.putText(frame, f'{ext}', (80, 40), cv2.FONT_HERSHEY_TRIPLEX, 1.0, blaсk_color, 3)\n",
        "\n",
        "        # Write the frame to the output video file\n",
        "        out.write(frame)\n",
        "\n",
        "        # # Show the frame\n",
        "        # cv2.imshow(\"Video\", og_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "8O8q15_s3N_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f91599db-b604-4e7c-ccab-35b4a2fd8d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 10 persons, 66.9ms\n",
            "Speed: 1.9ms preprocess, 66.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 6.8ms\n",
            "Speed: 3.9ms preprocess, 6.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 9.1ms\n",
            "Speed: 5.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 0\n",
            "fps 0\n",
            "fps 0\n",
            "fps 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 10 persons, 10.4ms\n",
            "Speed: 3.0ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 8.4ms\n",
            "Speed: 3.0ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.8ms\n",
            "Speed: 4.9ms preprocess, 8.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 8.5ms\n",
            "Speed: 3.2ms preprocess, 8.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 0\n",
            "fps 0\n",
            "fps 0\n",
            "fps 7.621968691365138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 10 persons, 10.5ms\n",
            "Speed: 3.1ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.5ms\n",
            "Speed: 4.3ms preprocess, 8.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 8.0ms\n",
            "Speed: 4.5ms preprocess, 8.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 8.3ms\n",
            "Speed: 3.8ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 10 persons, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.4ms\n",
            "Speed: 3.8ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 9.1ms\n",
            "Speed: 2.7ms preprocess, 9.1ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 8.8ms\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 8 persons, 8.4ms\n",
            "Speed: 3.3ms preprocess, 8.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 10.4ms\n",
            "Speed: 2.8ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.2ms\n",
            "Speed: 2.5ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 9 persons, 18.8ms\n",
            "Speed: 2.8ms preprocess, 18.8ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 16.9ms\n",
            "Speed: 2.3ms preprocess, 16.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 24.7ms\n",
            "Speed: 2.7ms preprocess, 24.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n",
            "fps 7.621968691365138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 10 persons, 14.0ms\n",
            "Speed: 6.9ms preprocess, 14.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 9.4ms\n",
            "Speed: 5.0ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 12.9ms\n",
            "Speed: 2.5ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 11.4ms\n",
            "Speed: 4.1ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 9 persons, 20.8ms\n",
            "Speed: 4.7ms preprocess, 20.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 18.7ms\n",
            "Speed: 2.4ms preprocess, 18.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 11 persons, 25.7ms\n",
            "Speed: 7.2ms preprocess, 25.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 24.8ms\n",
            "Speed: 6.2ms preprocess, 24.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 25.2ms\n",
            "Speed: 7.0ms preprocess, 25.2ms inference, 10.3ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 11 persons, 18.5ms\n",
            "Speed: 2.4ms preprocess, 18.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 15.4ms\n",
            "Speed: 4.3ms preprocess, 15.4ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n",
            "fps 15.252179899938124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 12 persons, 16.5ms\n",
            "Speed: 4.7ms preprocess, 16.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 21.2ms\n",
            "Speed: 7.7ms preprocess, 21.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 12.9ms\n",
            "Speed: 4.4ms preprocess, 12.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.557357066164938\n",
            "fps 10.557357066164938\n",
            "fps 10.557357066164938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 11 persons, 17.2ms\n",
            "Speed: 4.5ms preprocess, 17.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 19.1ms\n",
            "Speed: 2.4ms preprocess, 19.1ms inference, 8.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 12 persons, 19.3ms\n",
            "Speed: 5.7ms preprocess, 19.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.557357066164938\n",
            "fps 10.557357066164938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 11 persons, 15.2ms\n",
            "Speed: 4.5ms preprocess, 15.2ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 17.9ms\n",
            "Speed: 4.7ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.557357066164938\n",
            "fps 10.557357066164938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 12 persons, 27.3ms\n",
            "Speed: 5.2ms preprocess, 27.3ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.557357066164938\n",
            "fps 10.557357066164938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 11 persons, 27.1ms\n",
            "Speed: 11.6ms preprocess, 27.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 23.4ms\n",
            "Speed: 8.4ms preprocess, 23.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.7ms\n",
            "Speed: 3.3ms preprocess, 8.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 8.3ms\n",
            "Speed: 3.4ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.557357066164938\n",
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 10 persons, 8.2ms\n",
            "Speed: 3.0ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 16.1ms\n",
            "Speed: 5.1ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 10.1ms\n",
            "Speed: 2.5ms preprocess, 10.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 10 persons, 9.7ms\n",
            "Speed: 2.6ms preprocess, 9.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 10 persons, 17.9ms\n",
            "Speed: 4.5ms preprocess, 17.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 10 persons, 14.9ms\n",
            "Speed: 2.6ms preprocess, 14.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 17.3ms\n",
            "Speed: 4.5ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 persons, 31.1ms\n",
            "Speed: 2.6ms preprocess, 31.1ms inference, 8.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 9.310246695980346\n",
            "fps 9.310246695980346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 10 persons, 27.9ms\n",
            "Speed: 8.6ms preprocess, 27.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 9.310246695980346\n",
            "fps 10.970527327278564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 9 persons, 35.8ms\n",
            "Speed: 4.1ms preprocess, 35.8ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 12.5ms\n",
            "Speed: 5.4ms preprocess, 12.5ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 25.1ms\n",
            "Speed: 13.7ms preprocess, 25.1ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.970527327278564\n",
            "fps 10.970527327278564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 7 persons, 16.9ms\n",
            "Speed: 5.0ms preprocess, 16.9ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 14.3ms\n",
            "Speed: 9.9ms preprocess, 14.3ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.970527327278564\n",
            "fps 10.970527327278564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 8 persons, 22.6ms\n",
            "Speed: 5.0ms preprocess, 22.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 20.4ms\n",
            "Speed: 5.9ms preprocess, 20.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.970527327278564\n",
            "fps 10.970527327278564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 8 persons, 9.5ms\n",
            "Speed: 3.8ms preprocess, 9.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 27.6ms\n",
            "Speed: 2.6ms preprocess, 27.6ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.970527327278564\n",
            "fps 10.970527327278564\n",
            "fps 10.970527327278564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 7 persons, 47.8ms\n",
            "Speed: 2.5ms preprocess, 47.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 persons, 14.3ms\n",
            "Speed: 2.4ms preprocess, 14.3ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 persons, 33.4ms\n",
            "Speed: 2.5ms preprocess, 33.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 8 persons, 10.2ms\n",
            "Speed: 2.7ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 8.8ms\n",
            "Speed: 3.0ms preprocess, 8.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 8 persons, 15.2ms\n",
            "Speed: 7.5ms preprocess, 15.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 31.5ms\n",
            "Speed: 2.4ms preprocess, 31.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 29.4ms\n",
            "Speed: 3.7ms preprocess, 29.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 8 persons, 23.7ms\n",
            "Speed: 4.4ms preprocess, 23.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 13.8ms\n",
            "Speed: 5.2ms preprocess, 13.8ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n",
            "fps 8.817007043770424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 7 persons, 16.4ms\n",
            "Speed: 5.2ms preprocess, 16.4ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 17.9ms\n",
            "Speed: 2.4ms preprocess, 17.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 17.4ms\n",
            "Speed: 6.3ms preprocess, 17.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 8.817007043770424\n",
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 7 persons, 22.0ms\n",
            "Speed: 2.6ms preprocess, 22.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 12.5ms\n",
            "Speed: 6.6ms preprocess, 12.5ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 8.8ms\n",
            "Speed: 2.4ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 6 persons, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 9.1ms\n",
            "Speed: 2.3ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 8.3ms\n",
            "Speed: 2.3ms preprocess, 8.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 16.2ms\n",
            "Speed: 2.6ms preprocess, 16.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 8.4ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Speed: 3.4ms preprocess, 8.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 8.5ms\n",
            "Speed: 3.7ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 26.1ms\n",
            "Speed: 8.1ms preprocess, 26.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0: 384x640 7 persons, 15.5ms\n",
            "Speed: 5.7ms preprocess, 15.5ms inference, 8.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 17.1ms\n",
            "Speed: 4.6ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 17.6ms\n",
            "Speed: 5.2ms preprocess, 17.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps 10.542653664192104\n",
            "fps 10.542653664192104\n",
            "fps 13.333875476011398\n",
            "fps 13.333875476011398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 5 persons, 8.9ms\n",
            "Speed: 2.5ms preprocess, 8.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1aa600689532>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# object tracking DeepSORT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtracks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes_xywh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# parsing tracking parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Object_Tracking_DeepSORT_YOLOv8/deep_sort/deep_sort.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, bbox_xywh, confidences, ori_img)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mori_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# generate detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_xywh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mbbox_tlwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xywh_to_tlwh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_xywh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         detections = [Detection(bbox_tlwh[i], conf, features[i]) for i, conf in enumerate(\n",
            "\u001b[0;32m/content/Object_Tracking_DeepSORT_YOLOv8/deep_sort/deep_sort.py\u001b[0m in \u001b[0;36m_get_features\u001b[0;34m(self, bbox_xywh, ori_img)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mim_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim_crops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Object_Tracking_DeepSORT_YOLOv8/deep_sort/deep/feature_extractor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, im_crops)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mim_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mim_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Object_Tracking_DeepSORT_YOLOv8/deep_sort/deep/feature_extractor.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(self, im_crops)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         im_batch = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(\n\u001b[0m\u001b[1;32m     39\u001b[0m             0) for im in im_crops], dim=0).float()\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Object_Tracking_DeepSORT_YOLOv8/deep_sort/deep/feature_extractor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         im_batch = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(\n\u001b[0m\u001b[1;32m     39\u001b[0m             0) for im in im_crops], dim=0).float()\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load file mp4\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "output_path = '/content/Tracking-and-counting-Using-YOLOv8-and-DeepSORT/output.mp4'\n",
        "# output_path = '/content/test3.mp4'\n",
        "mp4 = open(output_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()"
      ],
      "metadata": {
        "id": "Po_p6p6J7qsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# video\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n"
      ],
      "metadata": {
        "id": "o0nxxjo27qwe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}